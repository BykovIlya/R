print(paste0('this is loop number ',i))
hold <- c(hold,i)
print(hold)
}
holder <- NULL
for (i in target_urls){
dat <- getData(i)
holder <- rbind(holder,dat)
}
holder
targets <- 201401:201406
targets <- paste("http://stats.grok.se/json/en/",
201401:201406,"/web_scraping",sep="")
dat <- ldply(targets,getData)
targets <- c("Barack_Obama","United_States_elections,_2014")
target_urls <- paste("http://stats.grok.se/json/en/201401/",targets,sep="")
results <- ldply(target_urls,getData)
#find number of rows for each:
t <- nrow(results)/length(targets)
t
results$id <- rep(targets,each=t)
url <- 'http://www.dailymail.co.uk/reader-comments/p/asset/readcomments/2643770?max=10&order=desc'
raw.data <- readLines(url, warn="F")
rd  <- fromJSON(raw.data)
str(rd)
dat <- rd$payload$page
dat$replies <- NULL
head(dat)
# Init packages
library("rvest")
library("shiny")
url <- "http://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_population"
population <- url %>%
html() %>%
html_nodes(xpath='//*[@id="mw-content-text"]/table[1]') %>%
html_table()
population <- population[[1]]
head(population)
lego_movie <- html("http://www.imdb.com/title/tt1490017/")
lego_movie %>%
html_node("strong span") %>%
html_text() %>%
as.numeric()
lego_movie %>%
html_nodes("#titleCast .itemprop span") %>%
html_text()
apple <- html ("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8")
apple %>%
html_nodes(.top-level-genre") %>%
html_text()
apple <- html ("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8")
apple %>%
html_nodes(.top-level-genre") %>%
html_text()
apple <- html ("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8")
apple <- url %>%
html() %>%
html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "top-level-genre", " " ))]') %>%
html_table()
apple <- apple[[1]]
apple <- html ("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8")
categories<-apple %>%
html() %>%
html_nodes(".top-level-genre") %>%
html_text()
categories
apple <- html ("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8")
categories<-apple %>%
html() %>%
html_nodes("#selectedcontent a") %>%
html_text()
categories
apple <- html ("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8")
categories<-apple %>%
html() %>%
html_nodes(".top-level-genre") %>%
html_tag()
categories
apple <- html ("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8")
categories<-apple %>%
html() %>%
html_nodes(".top-level-genre") %>%
html_text(categories)
html_tag(categories)
html_attrs(categories)
categories
categories
apple <- html ("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8")
categories<-apple %>%
html() %>%
html_nodes(".top-level-genre") %>%
html_tag(categories)
categories
s <- html_session("http://had.co.nz")
s %>% jump_to("thesis/")
s %>% follow_link("vita")
s %>% follow_link(3)
s %>% follow_link("vita")
lego_movie %>%
html_nodes("#titleCast .itemprop span") %>%
html_text()
lego_movie %>%
html_nodes("table") %>%
.[[3]] %>%
html_table()
url <- "http://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_population"
population <- url %>%
html() %>%
html_nodes(xpath='//*[@id="mw-content-text"]/table[1]') %>%
html_table()
population <- population[[1]]
head(population)
webpage <- html("http://www.reed.edu/ir/geographic_states.html")
# the data we want is in the first table on this page
# the html_table() command coerces the data into a data frame
webpage %>%
html_nodes("table") %>%
.[[1]] %>%
html_table()
url <- "http://stackoverflow.com/questions/3746256/extract-links-from-webpage-using-r"
doc <- htmlParse(url)
links <- xpathSApply(doc, "//a/@href")
free(doc)
library(XML)
url <- "http://stackoverflow.com/questions/3746256/extract-links-from-webpage-using-r"
doc <- htmlParse(url)
links <- xpathSApply(doc, "//a/@href")
free(doc)
doc
library(stringr)
url <- "http://stackoverflow.com/questions/3746256/extract-links-from-webpage-using-r"
html <- paste(readLines(url), collapse="\n")
matched <- str_match_all(html, "<a href=\"(.*?)\"")
links <- matched[[1]][, 2]
head(links)
library(XML)
theurl <- "http://en.wikipedia.org/wiki/Brazil_national_football_team"
tables <- readHTMLTable(theurl)
n.rows <- unlist(lapply(tables, function(t) dim(t)[1]))
the picked table is the longest one on the page
tables[[which.max(n.rows)]]
library(XML)
PATH = "/colleges/Bentley-University"
URL <- paste("http://www.cappex.com", PATH, sep="")
doc <- htmlParse(URL)
saveXML(doc, file="ex.txt")
rm(list=ls())
# recover
doc<-htmlParse('ex.txt')
doc <- htmlParse(URL)
library(XML)
PATH = "/colleges/Bentley-University"
URL <- paste("http://www.cappex.com", PATH, sep="")
doc <- htmlParse(URL)
saveXML(doc, file="ex.txt")
rm(list=ls())
# recover
doc<-htmlParse('ex.txt')
library(stringr)
library(RCurl)
library(httr)
library(stringr)
library(RCurl)
library(httr)
library(XML)
PATH = "/colleges/Bentley-University"
URL <- paste("http://www.cappex.com", PATH, sep="")
doc <- htmlParse(URL)
saveXML(doc, file="ex.txt")
# recover
doc<-htmlParse('ex.txt')
url <- "http://finance.yahoo.com/q/op?s=DIA&m=2013-07"
tabs <- getURL(url)
tabs <- readHTMLTable(tabs, stringsAsFactors = F)
tabs
apple <- html ("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8")
apple <- html ("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8")
html <- paste(readLines(url), collapse="\n")
html <- paste(readLines(apple), collapse="\n")
html <- paste(readLines(apple))
matched <- str_match_all(html, "<a href=\"(.*?)\"")
links <- matched[[1]][, 2]
head(links)
html <- paste(readLines("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8"), collapse="\n")
matched <- str_match_all(html, "<a href=\"(.*?)\"")
links <- matched[[1]][, 2]
head(links)
APPLE = xmlParse("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8", isHTML = TRUE)
class(APPLE)
APPLE = htmlParse("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8", isHTML = TRUE)
APPLE = htmlParse("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8")
APPLE = xmlParse("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8", isHTML = TRUE)
class(APPLE)
APPLE = xmlParse("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8", isHTML = TRUE)
class(APPLE)
library("Ecdat")
data()Griliches
data(Griliches)
data(Griliches)
a < - data(Griliches)
a <- data(Griliches)
library("lubridate") # работа с датами
library("sandwich") # vcovHC, vcovHAC
library("lmtest") # тесты
library("car") # еще тесты
library("zoo") # временные ряды
library("xts") # еще ряды
library("dplyr") # манипуляции с данными
library("ggplot2") # графики
library("broom") # манипуляции
library("quantmod") # загрузка с finance.google.com
library("rusquant") # загрузка с finam.ru
library("sophisthse") # загрузка с sophist.hse.ru
library("Quandl") # загрузка с Quandl
# задаём даты в виде простого текста
x <- c("2012-04-15","2011-08-17")
y <- ymd(x) # конвертируем в специальный формат дат
y
y + days(20) # прибавим 20 дней
y - years(10) # вычтем 10 лет
day(y) # вытащим из даты только число
month(y) # ... только месяц
year(y) # ... только год
vignette("lubridate") # более подробная справка про даты
# создадим временной ряд
x <- rnorm(5) # пять N(0,1) случайных величин
x
y <- ymd("2014-01-01")+days(0:4) # даты к этим величинам
y
ts <- zoo(x,order.by=y) # склеим числа и даты в один временной ряд
ts
lag(ts,-1) # лаг, то есть прошлое значение ряда
lag(ts,1) # форвардный лаг, то есть будущее значение
diff(ts) # приращение ряда
# те же пять чисел, только оформленные как квартальные данные
ts2 <- zooreg(x,start=as.yearqtr("2014-01"),freq=4)
ts2
# те же пять чисел, только оформленные как месячные данные
ts3 <- zooreg(x,start=as.yearmon("2014-01"),freq=12)
ts3
data("Investment") # встроенный набор данных
help("Investment")
start(Investment) # момент начала временного ряда
end(Investment) # окончания
time(Investment) # только моменты времени
coredata(Investment) # только сами числа без дат
dna <- Investment # скопируем набор данных Investment
dna[1,2] <- NA # и внесем туда искусственно пропуски
dna[5,3] <- NA
na.approx(dna) # линейная аппроксимация
na.locf(dna) # заполнение последним известным значением
# загрузка данных с sophist.hse.ru
# это численность населения России
a <- sophisthse("POPNUM_Y")
a
# другие названия рядов можно глянуть
#на http://sophist.hse.ru/hse/nindex.shtml
# например, CPI_Y_CHI --- индекс потребительских цен
# загрузка данных с quandl
b <- Quandl("FRED/GNP")
b
# это огромная база, по ней есть удобный поиск
# https://www.quandl.com/
# загрузка данных finance.google.com
Sys.setlocale("LC_TIME","C") # это шаманское заклинание позволяет избежать проблем с русской кодировкой месяцев под windows
# цены акций компании Apple:
getSymbols(Symbols = "AAPL",from="2010-01-01",
to="2014-02-03",src="google")
head(AAPL)
tail(AAPL)
# загрузка данных с finam.ru
# цены акций компании Газпром
getSymbols(Symbols="GAZP",from="2011-01-02",
to="2014-09-09",src="Finam")
head(GAZP)
tail(GAZP)
# несколько вариантов графиков:
plot(GAZP)
autoplot(GAZP[,1:4])
autoplot(GAZP[,1:4],facets = NULL)
chartSeries(GAZP)
# возвращаемся к набору данных с инвестициями
# в R есть два популярных формата хранения табличных данных
# это data.frame для невременных рядов
# и zoo или xts для временных рядов
# некоторые функции заточены под один формат, некоторые - под другой
# мы превращаем data.frame Investment в zoo временной ряд
d <- as.zoo(Investment)
autoplot(d[,1:2],facets = NULL)
# простая линейная модель
model <- lm(data=d, RealInv~RealInt+RealGNP)
summary(model) # краткий отчет по модели
coeftest(model) # тесты на коэффициенты
confint(model) # доверительные интервалы для коэффициентов
# в этих трех командах по умолчанию используются
# некорректные для автокорреляции станадртные ошибки
# добавим к исходных данным остатки и прогнозы
glimpse(d_aug)
qplot(data=d_aug,lag(.resid),.resid) # график остатка от предыдущего значения
vcov(model) # обычная оценка ковариационной матрицы
# не годная в условиях автокорреляции
vcovHAC(model) # робастная оценка ковариационной матрицы
# годная в условиях автокорреляции
d_aug <- augment(model, as.data.frame(d))
# тестируем гипотезы о равенстве коэффициентов нулю
# с помощью правильной оценки ковариационной матрицы
coeftest(model,vcov. = vcovHAC(model))
# строим корректные при автокоррреляции доверительные интервалы
conftable <- coeftest(model,vcov. = vcovHAC(model))
ci <- data.frame(estimate=conftable[,1],
se_ac=conftable[,2])
ci <- mutate(ci,left_95=estimate-1.96*se_ac,
right_95=estimate+1.96*se_ac)
ci
# Durbin-Watson
# H0: нет автокорреляции
# Ha: автокорреляции 1-го порядка
dwt(model)
res <- dwt(model)
res$dw # значение статистики DW
res$p # симуляционное p-value.
# В силу небольшого количества наблюдений и симуляционных свойств алгоритма может колебаться
res$r # оценка корреляции
# Тест Бройша-Годфри
# H0: нет автокорреляции
# Ha: автокорреляция k-го порядка
bgtest(model,order = 2)
# H0 не отвергается
res <- bgtest(model,order = 2)
res$statistic # значение статистики BG
res$p.value # P-значение
a <- data(Griliches)
model <- lm ( lw80 ~ age80 + iq + school80 + expr80)
head(a)
model <- lm ( lw80 ~ age80 + iq + school80 + expr80)
model <- lm (lw80 ~ age80 + iq + school80 + expr80, data = a)
a <- data(Griliches)
head(a)
a
data(Griliches)
Griliches
model <- lm (lw80 ~ age80 + iq + school80 + expr80, data = Griliches)
summary(model)
cov(model)
View(Griliches)
cov(Griliches)
cov(Griliches)
View(Griliches)
vcov(model)
vcovHC(model,type="HC0") # формула Уайта
vcovHC(model) # современный вариант формулы Уайта "HC3"
vcovHC(model,type="HC2") # еще один вариант
vcovHC(model) # современный вариант формулы Уайта "HC3"
coeftest(model) # обычной оценки ковариационной матрицы
5.926405e-07 - 7.155278e-07
bptest(model)
# тест Голдфельда-Квандта
gqtest(model, order.by = ~expr80, data=Griliches, fraction = 0.2)
data(Solow)
model2 <- lm (q ~ k + A, data = Solow)
summary(model)
bgtest(model2,order = 2)
bgtest(model2,order = 3)
Sys.setlocale("LC_TIME","C") # это шаманское заклинание позволяет избежать проблем с русской кодировкой месяцев под windows
# цены акций компании Apple:
getSymbols(Symbols = "AAPL",from="2010-01-01",
to="2014-02-03",src="google")
head(AAPL)
tail(AAPL)
# Esli russkie bukvi prevratilitis v krakozyabry,
# to File - Reopen with encoding... - UTF-8 - Set as default - OK
library("lubridate") # работа с датами
library("sandwich") # vcovHC, vcovHAC
library("lmtest") # тесты
library("car") # еще тесты
library("zoo") # временные ряды
library("xts") # еще ряды
library("dplyr") # манипуляции с данными
library("broom") # манипуляции
library("ggplot2") # графики
library("quantmod") # загрузка с finance.google.com
library("rusquant") # загрузка с finam.ru
library("sophisthse") # загрузка с sophist.hse.ru
library("Quandl") # загрузка с Quandl
# задаём даты в виде простого текста
x <- c("2012-04-15","2011-08-17")
y <- ymd(x) # конвертируем в специальный формат дат
y
y + days(20) # прибавим 20 дней
y - years(10) # вычтем 10 лет
day(y) # вытащим из даты только число
month(y) # ... только месяц
year(y) # ... только год
vignette("lubridate") # более подробная справка про даты
# создадим временной ряд
x <- rnorm(5) # пять N(0,1) случайных величин
x
y <- ymd("2014-01-01")+days(0:4) # даты к этим величинам
y
ts <- zoo(x,order.by=y) # склеим числа и даты в один временной ряд
ts
lag(ts,-1) # лаг, то есть прошлое значение ряда
lag(ts,1) # форвардный лаг, то есть будущее значение
diff(ts) # приращение ряда
# те же пять чисел, только оформленные как квартальные данные
ts2 <- zooreg(x,start=as.yearqtr("2014-01"),freq=4)
ts2
# те же пять чисел, только оформленные как месячные данные
ts3 <- zooreg(x,start=as.yearmon("2014-01"),freq=12)
ts3
data("Investment") # встроенный набор данных
help("Investment")
start(Investment) # момент начала временного ряда
end(Investment) # окончания
time(Investment) # только моменты времени
coredata(Investment) # только сами числа без дат
dna <- Investment # скопируем набор данных Investment
dna[1,2] <- NA # и внесем туда искусственно пропуски
dna[5,3] <- NA
na.approx(dna) # линейная аппроксимация
na.locf(dna) # заполнение последним известным значением
# загрузка данных с sophist.hse.ru
# это численность населения России
a <- sophisthse("POPNUM_Y")
a
# другие названия рядов можно глянуть
#на http://sophist.hse.ru/hse/nindex.shtml
# например, CPI_Y_CHI --- индекс потребительских цен
# загрузка данных с quandl
b <- Quandl("FRED/GNP")
b
# это огромная база, по ней есть удобный поиск
# https://www.quandl.com/
# загрузка данных finance.google.com
Sys.setlocale("LC_TIME","C") # это шаманское заклинание позволяет избежать проблем с русской кодировкой месяцев под windows
# цены акций компании Apple:
getSymbols(Symbols = "AAPL",from="2010-01-01",
to="2014-02-03",src="google")
head(AAPL)
tail(AAPL)
Sys.setlocale("LC_TIME","C")
getSymbols(Symbols = "INTC",from="2010-01-01", to="2014-02-03",src="google")
plot(INTC$INTC.Close, main = "")
Sys.setlocale("LC_TIME","C")
getSymbols(Symbols = "AAPL",from="2010-01-01", to="2014-02-03",src="google")
plot(AAPL$AAPL.Close, main = "")
# загрузка данных с finam.ru
# цены акций компании Газпром
getSymbols(Symbols="GAZP",from="2011-01-02",
to="2014-09-09",src="Finam")
head(GAZP)
tail(GAZP)
# несколько вариантов графиков:
plot(GAZP)
autoplot(GAZP[,1:4])
autoplot(GAZP[,1:4],facets = NULL)
chartSeries(GAZP)
# возвращаемся к набору данных с инвестициями
# в R есть два популярных формата хранения табличных данных
# это data.frame для невременных рядов
# и zoo или xts для временных рядов
# некоторые функции заточены под один формат, некоторые - под другой
# мы превращаем data.frame Investment в zoo временной ряд
d <- as.zoo(Investment)
autoplot(d[,1:2],facets = NULL)
# простая линейная модель
model <- lm(data=d, RealInv~RealInt+RealGNP)
summary(model) # краткий отчет по модели
coeftest(model) # тесты на коэффициенты
confint(model) # доверительные интервалы для коэффициентов
# в этих трех командах по умолчанию используются
# некорректные для автокорреляции станадртные ошибки
# добавим к исходных данным остатки и прогнозы
d_aug <- augment(model, as.data.frame(d))
glimpse(d_aug)
qplot(data=d_aug,lag(.resid),.resid) # график остатка от предыдущего значения
vcov(model) # обычная оценка ковариационной матрицы
# не годная в условиях автокорреляции
vcovHAC(model) # робастная оценка ковариационной матрицы
# годная в условиях автокорреляции
# тестируем гипотезы о равенстве коэффициентов нулю
# с помощью правильной оценки ковариационной матрицы
coeftest(model,vcov. = vcovHAC(model))
# строим корректные при автокоррреляции доверительные интервалы
conftable <- coeftest(model,vcov. = vcovHAC(model))
ci <- data.frame(estimate=conftable[,1],
se_ac=conftable[,2])
ci <- mutate(ci,left_95=estimate-1.96*se_ac,
right_95=estimate+1.96*se_ac)
ci
# Durbin-Watson
# H0: нет автокорреляции
# Ha: автокорреляции 1-го порядка
dwt(model)
res <- dwt(model)
res$dw # значение статистики DW
res$p # симуляционное p-value.
# В силу небольшого количества наблюдений и симуляционных свойств алгоритма может колебаться
res$r # оценка корреляции
# Тест Бройша-Годфри
# H0: нет автокорреляции
# Ha: автокорреляция k-го порядка
bgtest(model2,order = 3)
# H0 не отвергается
res <- bgtest(model2,order = 2)
res$statistic # значение статистики BG
res$p.value # P-значение
setwd("~/GitHub/R/logit_probit")
setwd("C:/Users/Екатерина/Documents/GitHub/R/logit_probit")
rmarkdown::render("C:/Users/Екатерина/Documents/GitHub/R/logit_probit/logit_probit.Rmd")
rmarkdown::render("C:/Users/Екатерина/Documents/GitHub/R/logit_probit/logit_probit.Rmd")
rmarkdown::render("C:/Users/Екатерина/Documents/GitHub/R/logit_probit/logit_probit.Rmd")
