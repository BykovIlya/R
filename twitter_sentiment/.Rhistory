doc<-htmlParse('ex.txt')
url <- "http://finance.yahoo.com/q/op?s=DIA&m=2013-07"
tabs <- getURL(url)
tabs <- readHTMLTable(tabs, stringsAsFactors = F)
tabs
apple <- html ("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8")
apple <- html ("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8")
html <- paste(readLines(url), collapse="\n")
html <- paste(readLines(apple), collapse="\n")
html <- paste(readLines(apple))
matched <- str_match_all(html, "<a href=\"(.*?)\"")
links <- matched[[1]][, 2]
head(links)
html <- paste(readLines("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8"), collapse="\n")
matched <- str_match_all(html, "<a href=\"(.*?)\"")
links <- matched[[1]][, 2]
head(links)
APPLE = xmlParse("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8", isHTML = TRUE)
class(APPLE)
APPLE = htmlParse("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8", isHTML = TRUE)
APPLE = htmlParse("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8")
APPLE = xmlParse("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8", isHTML = TRUE)
class(APPLE)
APPLE = xmlParse("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8", isHTML = TRUE)
class(APPLE)
library("Ecdat")
data()Griliches
data(Griliches)
data(Griliches)
a < - data(Griliches)
a <- data(Griliches)
library("lubridate") # работа с датами
library("sandwich") # vcovHC, vcovHAC
library("lmtest") # тесты
library("car") # еще тесты
library("zoo") # временные ряды
library("xts") # еще ряды
library("dplyr") # манипуляции с данными
library("ggplot2") # графики
library("broom") # манипуляции
library("quantmod") # загрузка с finance.google.com
library("rusquant") # загрузка с finam.ru
library("sophisthse") # загрузка с sophist.hse.ru
library("Quandl") # загрузка с Quandl
# задаём даты в виде простого текста
x <- c("2012-04-15","2011-08-17")
y <- ymd(x) # конвертируем в специальный формат дат
y
y + days(20) # прибавим 20 дней
y - years(10) # вычтем 10 лет
day(y) # вытащим из даты только число
month(y) # ... только месяц
year(y) # ... только год
vignette("lubridate") # более подробная справка про даты
# создадим временной ряд
x <- rnorm(5) # пять N(0,1) случайных величин
x
y <- ymd("2014-01-01")+days(0:4) # даты к этим величинам
y
ts <- zoo(x,order.by=y) # склеим числа и даты в один временной ряд
ts
lag(ts,-1) # лаг, то есть прошлое значение ряда
lag(ts,1) # форвардный лаг, то есть будущее значение
diff(ts) # приращение ряда
# те же пять чисел, только оформленные как квартальные данные
ts2 <- zooreg(x,start=as.yearqtr("2014-01"),freq=4)
ts2
# те же пять чисел, только оформленные как месячные данные
ts3 <- zooreg(x,start=as.yearmon("2014-01"),freq=12)
ts3
data("Investment") # встроенный набор данных
help("Investment")
start(Investment) # момент начала временного ряда
end(Investment) # окончания
time(Investment) # только моменты времени
coredata(Investment) # только сами числа без дат
dna <- Investment # скопируем набор данных Investment
dna[1,2] <- NA # и внесем туда искусственно пропуски
dna[5,3] <- NA
na.approx(dna) # линейная аппроксимация
na.locf(dna) # заполнение последним известным значением
# загрузка данных с sophist.hse.ru
# это численность населения России
a <- sophisthse("POPNUM_Y")
a
# другие названия рядов можно глянуть
#на http://sophist.hse.ru/hse/nindex.shtml
# например, CPI_Y_CHI --- индекс потребительских цен
# загрузка данных с quandl
b <- Quandl("FRED/GNP")
b
# это огромная база, по ней есть удобный поиск
# https://www.quandl.com/
# загрузка данных finance.google.com
Sys.setlocale("LC_TIME","C") # это шаманское заклинание позволяет избежать проблем с русской кодировкой месяцев под windows
# цены акций компании Apple:
getSymbols(Symbols = "AAPL",from="2010-01-01",
to="2014-02-03",src="google")
head(AAPL)
tail(AAPL)
# загрузка данных с finam.ru
# цены акций компании Газпром
getSymbols(Symbols="GAZP",from="2011-01-02",
to="2014-09-09",src="Finam")
head(GAZP)
tail(GAZP)
# несколько вариантов графиков:
plot(GAZP)
autoplot(GAZP[,1:4])
autoplot(GAZP[,1:4],facets = NULL)
chartSeries(GAZP)
# возвращаемся к набору данных с инвестициями
# в R есть два популярных формата хранения табличных данных
# это data.frame для невременных рядов
# и zoo или xts для временных рядов
# некоторые функции заточены под один формат, некоторые - под другой
# мы превращаем data.frame Investment в zoo временной ряд
d <- as.zoo(Investment)
autoplot(d[,1:2],facets = NULL)
# простая линейная модель
model <- lm(data=d, RealInv~RealInt+RealGNP)
summary(model) # краткий отчет по модели
coeftest(model) # тесты на коэффициенты
confint(model) # доверительные интервалы для коэффициентов
# в этих трех командах по умолчанию используются
# некорректные для автокорреляции станадртные ошибки
# добавим к исходных данным остатки и прогнозы
glimpse(d_aug)
qplot(data=d_aug,lag(.resid),.resid) # график остатка от предыдущего значения
vcov(model) # обычная оценка ковариационной матрицы
# не годная в условиях автокорреляции
vcovHAC(model) # робастная оценка ковариационной матрицы
# годная в условиях автокорреляции
d_aug <- augment(model, as.data.frame(d))
# тестируем гипотезы о равенстве коэффициентов нулю
# с помощью правильной оценки ковариационной матрицы
coeftest(model,vcov. = vcovHAC(model))
# строим корректные при автокоррреляции доверительные интервалы
conftable <- coeftest(model,vcov. = vcovHAC(model))
ci <- data.frame(estimate=conftable[,1],
se_ac=conftable[,2])
ci <- mutate(ci,left_95=estimate-1.96*se_ac,
right_95=estimate+1.96*se_ac)
ci
# Durbin-Watson
# H0: нет автокорреляции
# Ha: автокорреляции 1-го порядка
dwt(model)
res <- dwt(model)
res$dw # значение статистики DW
res$p # симуляционное p-value.
# В силу небольшого количества наблюдений и симуляционных свойств алгоритма может колебаться
res$r # оценка корреляции
# Тест Бройша-Годфри
# H0: нет автокорреляции
# Ha: автокорреляция k-го порядка
bgtest(model,order = 2)
# H0 не отвергается
res <- bgtest(model,order = 2)
res$statistic # значение статистики BG
res$p.value # P-значение
a <- data(Griliches)
model <- lm ( lw80 ~ age80 + iq + school80 + expr80)
head(a)
model <- lm ( lw80 ~ age80 + iq + school80 + expr80)
model <- lm (lw80 ~ age80 + iq + school80 + expr80, data = a)
a <- data(Griliches)
head(a)
a
data(Griliches)
Griliches
model <- lm (lw80 ~ age80 + iq + school80 + expr80, data = Griliches)
summary(model)
cov(model)
View(Griliches)
cov(Griliches)
cov(Griliches)
View(Griliches)
vcov(model)
vcovHC(model,type="HC0") # формула Уайта
vcovHC(model) # современный вариант формулы Уайта "HC3"
vcovHC(model,type="HC2") # еще один вариант
vcovHC(model) # современный вариант формулы Уайта "HC3"
coeftest(model) # обычной оценки ковариационной матрицы
5.926405e-07 - 7.155278e-07
bptest(model)
# тест Голдфельда-Квандта
gqtest(model, order.by = ~expr80, data=Griliches, fraction = 0.2)
data(Solow)
model2 <- lm (q ~ k + A, data = Solow)
summary(model)
bgtest(model2,order = 2)
bgtest(model2,order = 3)
Sys.setlocale("LC_TIME","C") # это шаманское заклинание позволяет избежать проблем с русской кодировкой месяцев под windows
# цены акций компании Apple:
getSymbols(Symbols = "AAPL",from="2010-01-01",
to="2014-02-03",src="google")
head(AAPL)
tail(AAPL)
# Esli russkie bukvi prevratilitis v krakozyabry,
# to File - Reopen with encoding... - UTF-8 - Set as default - OK
library("lubridate") # работа с датами
library("sandwich") # vcovHC, vcovHAC
library("lmtest") # тесты
library("car") # еще тесты
library("zoo") # временные ряды
library("xts") # еще ряды
library("dplyr") # манипуляции с данными
library("broom") # манипуляции
library("ggplot2") # графики
library("quantmod") # загрузка с finance.google.com
library("rusquant") # загрузка с finam.ru
library("sophisthse") # загрузка с sophist.hse.ru
library("Quandl") # загрузка с Quandl
# задаём даты в виде простого текста
x <- c("2012-04-15","2011-08-17")
y <- ymd(x) # конвертируем в специальный формат дат
y
y + days(20) # прибавим 20 дней
y - years(10) # вычтем 10 лет
day(y) # вытащим из даты только число
month(y) # ... только месяц
year(y) # ... только год
vignette("lubridate") # более подробная справка про даты
# создадим временной ряд
x <- rnorm(5) # пять N(0,1) случайных величин
x
y <- ymd("2014-01-01")+days(0:4) # даты к этим величинам
y
ts <- zoo(x,order.by=y) # склеим числа и даты в один временной ряд
ts
lag(ts,-1) # лаг, то есть прошлое значение ряда
lag(ts,1) # форвардный лаг, то есть будущее значение
diff(ts) # приращение ряда
# те же пять чисел, только оформленные как квартальные данные
ts2 <- zooreg(x,start=as.yearqtr("2014-01"),freq=4)
ts2
# те же пять чисел, только оформленные как месячные данные
ts3 <- zooreg(x,start=as.yearmon("2014-01"),freq=12)
ts3
data("Investment") # встроенный набор данных
help("Investment")
start(Investment) # момент начала временного ряда
end(Investment) # окончания
time(Investment) # только моменты времени
coredata(Investment) # только сами числа без дат
dna <- Investment # скопируем набор данных Investment
dna[1,2] <- NA # и внесем туда искусственно пропуски
dna[5,3] <- NA
na.approx(dna) # линейная аппроксимация
na.locf(dna) # заполнение последним известным значением
# загрузка данных с sophist.hse.ru
# это численность населения России
a <- sophisthse("POPNUM_Y")
a
# другие названия рядов можно глянуть
#на http://sophist.hse.ru/hse/nindex.shtml
# например, CPI_Y_CHI --- индекс потребительских цен
# загрузка данных с quandl
b <- Quandl("FRED/GNP")
b
# это огромная база, по ней есть удобный поиск
# https://www.quandl.com/
# загрузка данных finance.google.com
Sys.setlocale("LC_TIME","C") # это шаманское заклинание позволяет избежать проблем с русской кодировкой месяцев под windows
# цены акций компании Apple:
getSymbols(Symbols = "AAPL",from="2010-01-01",
to="2014-02-03",src="google")
head(AAPL)
tail(AAPL)
Sys.setlocale("LC_TIME","C")
getSymbols(Symbols = "INTC",from="2010-01-01", to="2014-02-03",src="google")
plot(INTC$INTC.Close, main = "")
Sys.setlocale("LC_TIME","C")
getSymbols(Symbols = "AAPL",from="2010-01-01", to="2014-02-03",src="google")
plot(AAPL$AAPL.Close, main = "")
# загрузка данных с finam.ru
# цены акций компании Газпром
getSymbols(Symbols="GAZP",from="2011-01-02",
to="2014-09-09",src="Finam")
head(GAZP)
tail(GAZP)
# несколько вариантов графиков:
plot(GAZP)
autoplot(GAZP[,1:4])
autoplot(GAZP[,1:4],facets = NULL)
chartSeries(GAZP)
# возвращаемся к набору данных с инвестициями
# в R есть два популярных формата хранения табличных данных
# это data.frame для невременных рядов
# и zoo или xts для временных рядов
# некоторые функции заточены под один формат, некоторые - под другой
# мы превращаем data.frame Investment в zoo временной ряд
d <- as.zoo(Investment)
autoplot(d[,1:2],facets = NULL)
# простая линейная модель
model <- lm(data=d, RealInv~RealInt+RealGNP)
summary(model) # краткий отчет по модели
coeftest(model) # тесты на коэффициенты
confint(model) # доверительные интервалы для коэффициентов
# в этих трех командах по умолчанию используются
# некорректные для автокорреляции станадртные ошибки
# добавим к исходных данным остатки и прогнозы
d_aug <- augment(model, as.data.frame(d))
glimpse(d_aug)
qplot(data=d_aug,lag(.resid),.resid) # график остатка от предыдущего значения
vcov(model) # обычная оценка ковариационной матрицы
# не годная в условиях автокорреляции
vcovHAC(model) # робастная оценка ковариационной матрицы
# годная в условиях автокорреляции
# тестируем гипотезы о равенстве коэффициентов нулю
# с помощью правильной оценки ковариационной матрицы
coeftest(model,vcov. = vcovHAC(model))
# строим корректные при автокоррреляции доверительные интервалы
conftable <- coeftest(model,vcov. = vcovHAC(model))
ci <- data.frame(estimate=conftable[,1],
se_ac=conftable[,2])
ci <- mutate(ci,left_95=estimate-1.96*se_ac,
right_95=estimate+1.96*se_ac)
ci
# Durbin-Watson
# H0: нет автокорреляции
# Ha: автокорреляции 1-го порядка
dwt(model)
res <- dwt(model)
res$dw # значение статистики DW
res$p # симуляционное p-value.
# В силу небольшого количества наблюдений и симуляционных свойств алгоритма может колебаться
res$r # оценка корреляции
# Тест Бройша-Годфри
# H0: нет автокорреляции
# Ha: автокорреляция k-го порядка
bgtest(model2,order = 3)
# H0 не отвергается
res <- bgtest(model2,order = 2)
res$statistic # значение статистики BG
res$p.value # P-значение
setwd("~/GitHub/R/twitter_sentiment")
setwd("C:/Users/Екатерина/Documents/GitHub/R/twitter_sentiment")
#install.packages(twitteR, dependencies = TRUE)
library("twitteR")
#install.packages("plyr", dependencies = TRUE)
library("plyr")
#install.packages("stringr", dependencies = TRUE)
library("stringr")
#install.packages("wordcloud", dependencies = TRUE)
library(wordcloud)
#install.packages("tm", dependencies = TRUE)
library(tm)
findScore <- function(sentence, pos.words, neg.words){
#uses words from corr.dictionaries
require("stringr")
#remove punctuation
sentence <- gsub("[[:punct:]]", "", sentence)
#remove control characters
sentence <- gsub("[[:cntrl:]]", "", sentence)
#remove digits
sentence <- gsub("\\d+", '', sentence)
tryTolower <- function(x) {
y <- NA
try_error <- tryCatch(tolower(x), error = function(e) e)
if(!inherits(try_error, "error"))
y <- tolower(x)
return(y)
}
sentence <- sapply(sentence, tryTolower)
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.indx <- pos.matches[!is.na(pos.matches)]
neg.indx <- neg.matches[!is.na(neg.matches)]
pos.matches = length(pos.indx)
neg.matches = length(neg.indx)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score <- sum(pos.matches) - sum(neg.matches)
return(list(score, pos.indx, neg.indx, sentence))
}
calcStats <- function(scores){
ntot <- length(scores)
npos <- sum(scores > 0)
nzero <- sum(scores == 0)
nneg <- sum(scores < 0)
if (npos != 0)
avpos <- sum(scores[scores > 0]) / npos
else avpos <- 0
if (nneg != 0)
avneg <- sum(scores[scores < 0]) / nneg
else avneg <- 0
return (c(ntot, npos, nzero, nneg, avpos, avneg))
}
setup_twitter_oauth("BfoP1LtBMdghX1ULzfHNI93zI","wNPYpa0hyRQHbAa5cce8KFhLz7AAArle7YK2XNt4vWU96YyYT2","168628773-lhgkkgzHG8R5ZJuagQuWnpPDyHZJmd5I4BIFwJGL","2gROpJPcJLXj1qRe3C7jK4fp7p7pzSJfkCCGU32cVjgRj")
findScore <- function(sentence, pos.words, neg.words){
#uses words from corr.dictionaries
require("stringr")
#remove punctuation
sentence <- gsub("[[:punct:]]", "", sentence)
#remove control characters
sentence <- gsub("[[:cntrl:]]", "", sentence)
#remove digits
sentence <- gsub("\\d+", '', sentence)
tryTolower <- function(x) {
y <- NA
try_error <- tryCatch(tolower(x), error = function(e) e)
if(!inherits(try_error, "error"))
y <- tolower(x)
return(y)
}
sentence <- sapply(sentence, tryTolower)
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.indx <- pos.matches[!is.na(pos.matches)]
neg.indx <- neg.matches[!is.na(neg.matches)]
pos.matches = length(pos.indx)
neg.matches = length(neg.indx)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score <- sum(pos.matches) - sum(neg.matches)
return(list(score, pos.indx, neg.indx, sentence))
}
calcStats <- function(scores){
ntot <- length(scores)
npos <- sum(scores > 0)
nzero <- sum(scores == 0)
nneg <- sum(scores < 0)
if (npos != 0)
avpos <- sum(scores[scores > 0]) / npos
else avpos <- 0
if (nneg != 0)
avneg <- sum(scores[scores < 0]) / nneg
else avneg <- 0
return (c(ntot, npos, nzero, nneg, avpos, avneg))
}
setup_twitter_oauth("BfoP1LtBMdghX1ULzfHNI93zI","wNPYpa0hyRQHbAa5cce8KFhLz7AAArle7YK2XNt4vWU96YyYT2","168628773-lhgkkgzHG8R5ZJuagQuWnpPDyHZJmd5I4BIFwJGL","2gROpJPcJLXj1qRe3C7jK4fp7p7pzSJfkCCGU32cVjgRj")
pos <- scan("positive-words.txt", what = "character")
neg <- scan("negative-words.txt", what = "character")
keywords <- "Russia"
nmax <- 180
location <- NULL
today <- Sys.Date()
since <- as.character(today)
until <- as.character(today + 1)
tweets <- searchTwitter(keywords, n = nmax,  since = since, until = until, geocode = location, lang = "en")
tweetdf <- data.frame()
tweetdf <- twListToDF(tweets)
scores <- laply(tweetdf$text, .fun = function(x)
{scl <- findScore(x, pos, neg); return(scl[[1]])})
stat1 <- calcStats(scores)
slices <- c(stat1[2], stat1[3],stat1[4])
lbls <- c("негативных", "нейтральных", "позитивных", stat1[2])
pie(slices, labels = lbls, main="Twitter hashtag sentiment")
findScore <- function(sentence, pos.words, neg.words){
#uses words from corr.dictionaries
require("stringr")
#remove punctuation
sentence <- gsub("[[:punct:]]", "", sentence)
#remove control characters
sentence <- gsub("[[:cntrl:]]", "", sentence)
#remove digits
sentence <- gsub("\\d+", '', sentence)
tryTolower <- function(x) {
y <- NA
try_error <- tryCatch(tolower(x), error = function(e) e)
if(!inherits(try_error, "error"))
y <- tolower(x)
return(y)
}
sentence <- sapply(sentence, tryTolower)
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.indx <- pos.matches[!is.na(pos.matches)]
neg.indx <- neg.matches[!is.na(neg.matches)]
pos.matches = length(pos.indx)
neg.matches = length(neg.indx)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score <- sum(pos.matches) - sum(neg.matches)
return(list(score, pos.indx, neg.indx, sentence))
}
calcStats <- function(scores){
ntot <- length(scores)
npos <- sum(scores > 0)
nzero <- sum(scores == 0)
nneg <- sum(scores < 0)
if (npos != 0)
avpos <- sum(scores[scores > 0]) / npos
else avpos <- 0
if (nneg != 0)
avneg <- sum(scores[scores < 0]) / nneg
else avneg <- 0
return (c(ntot, npos, nzero, nneg, avpos, avneg))
}
setup_twitter_oauth("BfoP1LtBMdghX1ULzfHNI93zI","wNPYpa0hyRQHbAa5cce8KFhLz7AAArle7YK2XNt4vWU96YyYT2","168628773-lhgkkgzHG8R5ZJuagQuWnpPDyHZJmd5I4BIFwJGL","2gROpJPcJLXj1qRe3C7jK4fp7p7pzSJfkCCGU32cVjgRj")
pos <- scan("positive-words.txt", what = "character")
neg <- scan("negative-words.txt", what = "character")
keywords <- "Russia"
nmax <- 180
location <- NULL
today <- Sys.Date()
since <- as.character(today)
until <- as.character(today + 1)
tweets <- searchTwitter(keywords, n = nmax,  since = since, until = until, geocode = location, lang = "en")
setup_twitter_oauth("xj5dwldlOwbxAy4GV0BFU3Y7C","emqDnvVHnxfWX2HpZ9K4VDarJvRQztwrpJvQ2aruKcydboAik1","168628773-lhgkkgzHG8R5ZJuagQuWnpPDyHZJmd5I4BIFwJGL","2gROpJPcJLXj1qRe3C7jK4fp7p7pzSJfkCCGU32cVjgRj")
tweets <- searchTwitter(keywords, n = nmax,  since = since, until = until, geocode = location, lang = "en")
