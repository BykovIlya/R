<<<<<<< Updated upstream
doc<-htmlParse('ex.txt')
url <- "http://finance.yahoo.com/q/op?s=DIA&m=2013-07"
tabs <- getURL(url)
tabs <- readHTMLTable(tabs, stringsAsFactors = F)
tabs
apple <- html ("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8")
apple <- html ("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8")
html <- paste(readLines(url), collapse="\n")
html <- paste(readLines(apple), collapse="\n")
html <- paste(readLines(apple))
matched <- str_match_all(html, "<a href=\"(.*?)\"")
links <- matched[[1]][, 2]
head(links)
html <- paste(readLines("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8"), collapse="\n")
matched <- str_match_all(html, "<a href=\"(.*?)\"")
links <- matched[[1]][, 2]
head(links)
APPLE = xmlParse("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8", isHTML = TRUE)
class(APPLE)
APPLE = htmlParse("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8", isHTML = TRUE)
APPLE = htmlParse("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8")
APPLE = xmlParse("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8", isHTML = TRUE)
class(APPLE)
APPLE = xmlParse("https://itunes.apple.com/ru/genre/ios-sport/id6004?mt=8", isHTML = TRUE)
class(APPLE)
library("Ecdat")
data()Griliches
data(Griliches)
data(Griliches)
a < - data(Griliches)
a <- data(Griliches)
library("lubridate") # работа с датами
library("sandwich") # vcovHC, vcovHAC
library("lmtest") # тесты
library("car") # еще тесты
=======
?rnorm
help.search("rmorm")
args("rnorm")
install.packages("swirl")
library("swirl")
swirl()
5+7
x <- 5 + 7
x
y <- x -3
y
z <- c(1.1, 9, 3.14)
?c
z
(z, 555, z)
(z,555,z)
(z 555 z)
(z, 555, z)
c(z, 555, z)
z * 2 + 100
my_sqrt <- sqrt(z -1)
my_sqrt
my_div <- (z/my_sqrt)
my_div <- z / my_sqrt
my_div
c(1, 2, 3, 4)
c(1, 2, 3, 4) + c(0, 10)
c(1, 2, 3, 4) + c(0, 10, 100)
(z * 2 + 1000)
(z * 2 + 1000)
info()
(z * 2 + 1000)
main()
z * 2 + 1000
my_div
swirl()
1:20
pi:10
15:1
':'
?`:`
seq(1, 20)
seq(0, 10, by=0.5)
my_seq <- seq(5, 10, length=30)
length(my_seq)
1:length(my_seq)
seq(along.with = my_seq)
seq_along(my_seq)
rep(0, times = 40)
rep(c(0, 1, 2), times = 10)
rep(c(0, 1, 2), each
| = 10)
rep(c(0, 1, 2), each = 10)
num_vect <- c(0.5, 55, -10, 6)
tf <- num_vect < 1
tf
num_vect >= 6
c("My", "name", "is")
my_char <- c("My", "name", "is")
my_char
paste(my_char, collapse = " ")
my_name <- c(my_char, "Kate")
my_name]
my_name
paste(my_name, collapse = " ")
paste("Hello", "world!", sep = " ")
paste(1:3, c("X", "Y", "Z"), , sep = " ")
paste(1:3, c("X", "Y", "Z"), , sep = " ")
paste(1:3, c("X", "Y", "Z"), , sep = "")
paste(c(1:3), c("X", "Y", "Z"), , sep = " ")
paste(1:3, c("X", "Y", "Z"), sep = "")
paste(LETTERS, 1:4, sep = "-")
x <- c(44, NA, 5, NA)
x*3
y <- rnorm(1000)
y
z <- rep(NA, 1000)
y_data <- sample(c(y, z), 100)
my_data <- sample(c(y, z), 100)
is.na(my_data)
my_na <- is.na(my_data)
my_na
my_data == NA
sum(my_na)
my_data
0/0
Inf/Inf
Inf - Inf
x
x[1:10]
x[is.na(x)]
y <- x[!is.na(x)]
y
y[y > 0]
x[x > 0]
x[!is.na(x) & x > 0]
c(3,5,7)
c(3, 5, 7)
[c(3, 5, 7)]
v <- c(3, 5, 7)
m <- c(3, 5, 7)
a <- c(3, 5, 7)
[f]
[a]
c[3, 5, 7]
c
x[c(3, 5, 7)]
x (i.e. x[0])
i.e. x[0]
x[0]
x[3000]
x[c(-2, -10)]
x[-c(2, 10)]
vect <- c(foo = 11, bar = 2, norf = NA)
vect
names(vect)
vect2 <- c(11, 2, NA)
names(vect2) <- c("foo", "bar", "norf")
identical(vect, vect2)
vect["bar"]
vect[c("foo", "bar")]
my_vector <- c(1:20)
my_vector <- 1:3
my_vector <- 1:20
my_vector
dim(my_vector)
length(my_vector)
dim(my_vector) <- c(4, 5)
v
dim(my_vector)
attributes(my_vector)
my_vector
class(my_vector)
my_matrix <- my_vector
?matrix()
?matrix
View(my_matrix)
View(my_matrix)
View(my_matrix)
View(my_vector)
View(my_vector)
my_matrix2 <- matrix(1:20, (dim(my_vector) <- c(4, 5))
setwd("~/Documents/GIT/R/twitter_sentiment")
install.packages(twitteR, dependencies = TRUE)
install.packages(twitteR, dependencies = TRUE)
install.packages(twitteR, dependencies = TRUE)
install.packages(twitteR, dependencies = TRUE)
library("lubridate") # работа с датами
>>>>>>> Stashed changes
library("zoo") # временные ряды
library("xts") # еще ряды
library("dplyr") # манипуляции с данными
library("ggplot2") # графики
<<<<<<< Updated upstream
library("broom") # манипуляции
library("quantmod") # загрузка с finance.google.com
library("rusquant") # загрузка с finam.ru
library("sophisthse") # загрузка с sophist.hse.ru
library("Quandl") # загрузка с Quandl
# задаём даты в виде простого текста
x <- c("2012-04-15","2011-08-17")
y <- ymd(x) # конвертируем в специальный формат дат
y
y + days(20) # прибавим 20 дней
y - years(10) # вычтем 10 лет
day(y) # вытащим из даты только число
month(y) # ... только месяц
year(y) # ... только год
vignette("lubridate") # более подробная справка про даты
# создадим временной ряд
x <- rnorm(5) # пять N(0,1) случайных величин
x
y <- ymd("2014-01-01")+days(0:4) # даты к этим величинам
y
ts <- zoo(x,order.by=y) # склеим числа и даты в один временной ряд
ts
lag(ts,-1) # лаг, то есть прошлое значение ряда
lag(ts,1) # форвардный лаг, то есть будущее значение
diff(ts) # приращение ряда
# те же пять чисел, только оформленные как квартальные данные
ts2 <- zooreg(x,start=as.yearqtr("2014-01"),freq=4)
ts2
# те же пять чисел, только оформленные как месячные данные
ts3 <- zooreg(x,start=as.yearmon("2014-01"),freq=12)
ts3
data("Investment") # встроенный набор данных
help("Investment")
start(Investment) # момент начала временного ряда
end(Investment) # окончания
time(Investment) # только моменты времени
coredata(Investment) # только сами числа без дат
dna <- Investment # скопируем набор данных Investment
dna[1,2] <- NA # и внесем туда искусственно пропуски
dna[5,3] <- NA
na.approx(dna) # линейная аппроксимация
na.locf(dna) # заполнение последним известным значением
# загрузка данных с sophist.hse.ru
# это численность населения России
a <- sophisthse("POPNUM_Y")
a
# другие названия рядов можно глянуть
#на http://sophist.hse.ru/hse/nindex.shtml
# например, CPI_Y_CHI --- индекс потребительских цен
# загрузка данных с quandl
b <- Quandl("FRED/GNP")
b
# это огромная база, по ней есть удобный поиск
# https://www.quandl.com/
# загрузка данных finance.google.com
Sys.setlocale("LC_TIME","C") # это шаманское заклинание позволяет избежать проблем с русской кодировкой месяцев под windows
# цены акций компании Apple:
getSymbols(Symbols = "AAPL",from="2010-01-01",
to="2014-02-03",src="google")
head(AAPL)
tail(AAPL)
# загрузка данных с finam.ru
# цены акций компании Газпром
getSymbols(Symbols="GAZP",from="2011-01-02",
to="2014-09-09",src="Finam")
head(GAZP)
tail(GAZP)
# несколько вариантов графиков:
plot(GAZP)
autoplot(GAZP[,1:4])
autoplot(GAZP[,1:4],facets = NULL)
chartSeries(GAZP)
# возвращаемся к набору данных с инвестициями
# в R есть два популярных формата хранения табличных данных
# это data.frame для невременных рядов
# и zoo или xts для временных рядов
# некоторые функции заточены под один формат, некоторые - под другой
# мы превращаем data.frame Investment в zoo временной ряд
d <- as.zoo(Investment)
autoplot(d[,1:2],facets = NULL)
# простая линейная модель
model <- lm(data=d, RealInv~RealInt+RealGNP)
summary(model) # краткий отчет по модели
coeftest(model) # тесты на коэффициенты
confint(model) # доверительные интервалы для коэффициентов
# в этих трех командах по умолчанию используются
# некорректные для автокорреляции станадртные ошибки
# добавим к исходных данным остатки и прогнозы
glimpse(d_aug)
qplot(data=d_aug,lag(.resid),.resid) # график остатка от предыдущего значения
vcov(model) # обычная оценка ковариационной матрицы
# не годная в условиях автокорреляции
vcovHAC(model) # робастная оценка ковариационной матрицы
# годная в условиях автокорреляции
d_aug <- augment(model, as.data.frame(d))
# тестируем гипотезы о равенстве коэффициентов нулю
# с помощью правильной оценки ковариационной матрицы
coeftest(model,vcov. = vcovHAC(model))
# строим корректные при автокоррреляции доверительные интервалы
conftable <- coeftest(model,vcov. = vcovHAC(model))
ci <- data.frame(estimate=conftable[,1],
se_ac=conftable[,2])
ci <- mutate(ci,left_95=estimate-1.96*se_ac,
right_95=estimate+1.96*se_ac)
ci
# Durbin-Watson
# H0: нет автокорреляции
# Ha: автокорреляции 1-го порядка
dwt(model)
res <- dwt(model)
res$dw # значение статистики DW
res$p # симуляционное p-value.
# В силу небольшого количества наблюдений и симуляционных свойств алгоритма может колебаться
res$r # оценка корреляции
# Тест Бройша-Годфри
# H0: нет автокорреляции
# Ha: автокорреляция k-го порядка
bgtest(model,order = 2)
# H0 не отвергается
res <- bgtest(model,order = 2)
res$statistic # значение статистики BG
res$p.value # P-значение
a <- data(Griliches)
model <- lm ( lw80 ~ age80 + iq + school80 + expr80)
head(a)
model <- lm ( lw80 ~ age80 + iq + school80 + expr80)
model <- lm (lw80 ~ age80 + iq + school80 + expr80, data = a)
a <- data(Griliches)
head(a)
a
data(Griliches)
Griliches
model <- lm (lw80 ~ age80 + iq + school80 + expr80, data = Griliches)
summary(model)
cov(model)
View(Griliches)
cov(Griliches)
cov(Griliches)
View(Griliches)
vcov(model)
vcovHC(model,type="HC0") # формула Уайта
vcovHC(model) # современный вариант формулы Уайта "HC3"
vcovHC(model,type="HC2") # еще один вариант
vcovHC(model) # современный вариант формулы Уайта "HC3"
coeftest(model) # обычной оценки ковариационной матрицы
5.926405e-07 - 7.155278e-07
bptest(model)
# тест Голдфельда-Квандта
gqtest(model, order.by = ~expr80, data=Griliches, fraction = 0.2)
data(Solow)
model2 <- lm (q ~ k + A, data = Solow)
summary(model)
bgtest(model2,order = 2)
bgtest(model2,order = 3)
Sys.setlocale("LC_TIME","C") # это шаманское заклинание позволяет избежать проблем с русской кодировкой месяцев под windows
# цены акций компании Apple:
getSymbols(Symbols = "AAPL",from="2010-01-01",
to="2014-02-03",src="google")
head(AAPL)
tail(AAPL)
# Esli russkie bukvi prevratilitis v krakozyabry,
# to File - Reopen with encoding... - UTF-8 - Set as default - OK
library("lubridate") # работа с датами
library("sandwich") # vcovHC, vcovHAC
library("lmtest") # тесты
library("car") # еще тесты
library("zoo") # временные ряды
library("xts") # еще ряды
library("dplyr") # манипуляции с данными
library("broom") # манипуляции
library("ggplot2") # графики
library("quantmod") # загрузка с finance.google.com
library("rusquant") # загрузка с finam.ru
library("sophisthse") # загрузка с sophist.hse.ru
library("Quandl") # загрузка с Quandl
# задаём даты в виде простого текста
x <- c("2012-04-15","2011-08-17")
y <- ymd(x) # конвертируем в специальный формат дат
y
y + days(20) # прибавим 20 дней
y - years(10) # вычтем 10 лет
day(y) # вытащим из даты только число
month(y) # ... только месяц
year(y) # ... только год
vignette("lubridate") # более подробная справка про даты
# создадим временной ряд
x <- rnorm(5) # пять N(0,1) случайных величин
x
y <- ymd("2014-01-01")+days(0:4) # даты к этим величинам
y
ts <- zoo(x,order.by=y) # склеим числа и даты в один временной ряд
ts
lag(ts,-1) # лаг, то есть прошлое значение ряда
lag(ts,1) # форвардный лаг, то есть будущее значение
diff(ts) # приращение ряда
# те же пять чисел, только оформленные как квартальные данные
ts2 <- zooreg(x,start=as.yearqtr("2014-01"),freq=4)
ts2
# те же пять чисел, только оформленные как месячные данные
ts3 <- zooreg(x,start=as.yearmon("2014-01"),freq=12)
ts3
data("Investment") # встроенный набор данных
help("Investment")
start(Investment) # момент начала временного ряда
end(Investment) # окончания
time(Investment) # только моменты времени
coredata(Investment) # только сами числа без дат
dna <- Investment # скопируем набор данных Investment
dna[1,2] <- NA # и внесем туда искусственно пропуски
dna[5,3] <- NA
na.approx(dna) # линейная аппроксимация
na.locf(dna) # заполнение последним известным значением
# загрузка данных с sophist.hse.ru
# это численность населения России
a <- sophisthse("POPNUM_Y")
a
# другие названия рядов можно глянуть
#на http://sophist.hse.ru/hse/nindex.shtml
# например, CPI_Y_CHI --- индекс потребительских цен
# загрузка данных с quandl
b <- Quandl("FRED/GNP")
b
# это огромная база, по ней есть удобный поиск
# https://www.quandl.com/
# загрузка данных finance.google.com
Sys.setlocale("LC_TIME","C") # это шаманское заклинание позволяет избежать проблем с русской кодировкой месяцев под windows
# цены акций компании Apple:
getSymbols(Symbols = "AAPL",from="2010-01-01",
to="2014-02-03",src="google")
head(AAPL)
tail(AAPL)
Sys.setlocale("LC_TIME","C")
getSymbols(Symbols = "INTC",from="2010-01-01", to="2014-02-03",src="google")
plot(INTC$INTC.Close, main = "")
Sys.setlocale("LC_TIME","C")
getSymbols(Symbols = "AAPL",from="2010-01-01", to="2014-02-03",src="google")
plot(AAPL$AAPL.Close, main = "")
# загрузка данных с finam.ru
# цены акций компании Газпром
getSymbols(Symbols="GAZP",from="2011-01-02",
to="2014-09-09",src="Finam")
head(GAZP)
tail(GAZP)
# несколько вариантов графиков:
plot(GAZP)
autoplot(GAZP[,1:4])
autoplot(GAZP[,1:4],facets = NULL)
chartSeries(GAZP)
# возвращаемся к набору данных с инвестициями
# в R есть два популярных формата хранения табличных данных
# это data.frame для невременных рядов
# и zoo или xts для временных рядов
# некоторые функции заточены под один формат, некоторые - под другой
# мы превращаем data.frame Investment в zoo временной ряд
d <- as.zoo(Investment)
autoplot(d[,1:2],facets = NULL)
# простая линейная модель
model <- lm(data=d, RealInv~RealInt+RealGNP)
summary(model) # краткий отчет по модели
coeftest(model) # тесты на коэффициенты
confint(model) # доверительные интервалы для коэффициентов
# в этих трех командах по умолчанию используются
# некорректные для автокорреляции станадртные ошибки
# добавим к исходных данным остатки и прогнозы
d_aug <- augment(model, as.data.frame(d))
glimpse(d_aug)
qplot(data=d_aug,lag(.resid),.resid) # график остатка от предыдущего значения
vcov(model) # обычная оценка ковариационной матрицы
# не годная в условиях автокорреляции
vcovHAC(model) # робастная оценка ковариационной матрицы
# годная в условиях автокорреляции
# тестируем гипотезы о равенстве коэффициентов нулю
# с помощью правильной оценки ковариационной матрицы
coeftest(model,vcov. = vcovHAC(model))
# строим корректные при автокоррреляции доверительные интервалы
conftable <- coeftest(model,vcov. = vcovHAC(model))
ci <- data.frame(estimate=conftable[,1],
se_ac=conftable[,2])
ci <- mutate(ci,left_95=estimate-1.96*se_ac,
right_95=estimate+1.96*se_ac)
ci
# Durbin-Watson
# H0: нет автокорреляции
# Ha: автокорреляции 1-го порядка
dwt(model)
res <- dwt(model)
res$dw # значение статистики DW
res$p # симуляционное p-value.
# В силу небольшого количества наблюдений и симуляционных свойств алгоритма может колебаться
res$r # оценка корреляции
# Тест Бройша-Годфри
# H0: нет автокорреляции
# Ha: автокорреляция k-го порядка
bgtest(model2,order = 3)
# H0 не отвергается
res <- bgtest(model2,order = 2)
res$statistic # значение статистики BG
res$p.value # P-значение
setwd("~/GitHub/R/twitter_sentiment")
setwd("C:/Users/Екатерина/Documents/GitHub/R/twitter_sentiment")
#install.packages(twitteR, dependencies = TRUE)
library("twitteR")
#install.packages("plyr", dependencies = TRUE)
library("plyr")
#install.packages("stringr", dependencies = TRUE)
library("stringr")
#install.packages("wordcloud", dependencies = TRUE)
library(wordcloud)
#install.packages("tm", dependencies = TRUE)
library(tm)
findScore <- function(sentence, pos.words, neg.words){
#uses words from corr.dictionaries
require("stringr")
#remove punctuation
sentence <- gsub("[[:punct:]]", "", sentence)
#remove control characters
sentence <- gsub("[[:cntrl:]]", "", sentence)
#remove digits
sentence <- gsub("\\d+", '', sentence)
tryTolower <- function(x) {
y <- NA
try_error <- tryCatch(tolower(x), error = function(e) e)
if(!inherits(try_error, "error"))
y <- tolower(x)
return(y)
}
sentence <- sapply(sentence, tryTolower)
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.indx <- pos.matches[!is.na(pos.matches)]
neg.indx <- neg.matches[!is.na(neg.matches)]
pos.matches = length(pos.indx)
neg.matches = length(neg.indx)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score <- sum(pos.matches) - sum(neg.matches)
return(list(score, pos.indx, neg.indx, sentence))
}
calcStats <- function(scores){
ntot <- length(scores)
npos <- sum(scores > 0)
nzero <- sum(scores == 0)
nneg <- sum(scores < 0)
if (npos != 0)
avpos <- sum(scores[scores > 0]) / npos
else avpos <- 0
if (nneg != 0)
avneg <- sum(scores[scores < 0]) / nneg
else avneg <- 0
return (c(ntot, npos, nzero, nneg, avpos, avneg))
}
setup_twitter_oauth("BfoP1LtBMdghX1ULzfHNI93zI","wNPYpa0hyRQHbAa5cce8KFhLz7AAArle7YK2XNt4vWU96YyYT2","168628773-lhgkkgzHG8R5ZJuagQuWnpPDyHZJmd5I4BIFwJGL","2gROpJPcJLXj1qRe3C7jK4fp7p7pzSJfkCCGU32cVjgRj")
findScore <- function(sentence, pos.words, neg.words){
#uses words from corr.dictionaries
require("stringr")
#remove punctuation
sentence <- gsub("[[:punct:]]", "", sentence)
#remove control characters
sentence <- gsub("[[:cntrl:]]", "", sentence)
#remove digits
sentence <- gsub("\\d+", '', sentence)
tryTolower <- function(x) {
y <- NA
try_error <- tryCatch(tolower(x), error = function(e) e)
if(!inherits(try_error, "error"))
y <- tolower(x)
return(y)
}
sentence <- sapply(sentence, tryTolower)
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.indx <- pos.matches[!is.na(pos.matches)]
neg.indx <- neg.matches[!is.na(neg.matches)]
pos.matches = length(pos.indx)
neg.matches = length(neg.indx)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score <- sum(pos.matches) - sum(neg.matches)
return(list(score, pos.indx, neg.indx, sentence))
}
calcStats <- function(scores){
ntot <- length(scores)
npos <- sum(scores > 0)
nzero <- sum(scores == 0)
nneg <- sum(scores < 0)
if (npos != 0)
avpos <- sum(scores[scores > 0]) / npos
else avpos <- 0
if (nneg != 0)
avneg <- sum(scores[scores < 0]) / nneg
else avneg <- 0
return (c(ntot, npos, nzero, nneg, avpos, avneg))
}
setup_twitter_oauth("BfoP1LtBMdghX1ULzfHNI93zI","wNPYpa0hyRQHbAa5cce8KFhLz7AAArle7YK2XNt4vWU96YyYT2","168628773-lhgkkgzHG8R5ZJuagQuWnpPDyHZJmd5I4BIFwJGL","2gROpJPcJLXj1qRe3C7jK4fp7p7pzSJfkCCGU32cVjgRj")
pos <- scan("positive-words.txt", what = "character")
neg <- scan("negative-words.txt", what = "character")
keywords <- "Russia"
nmax <- 180
location <- NULL
today <- Sys.Date()
since <- as.character(today)
until <- as.character(today + 1)
tweets <- searchTwitter(keywords, n = nmax,  since = since, until = until, geocode = location, lang = "en")
tweetdf <- data.frame()
tweetdf <- twListToDF(tweets)
scores <- laply(tweetdf$text, .fun = function(x)
{scl <- findScore(x, pos, neg); return(scl[[1]])})
stat1 <- calcStats(scores)
slices <- c(stat1[2], stat1[3],stat1[4])
lbls <- c("негативных", "нейтральных", "позитивных", stat1[2])
pie(slices, labels = lbls, main="Twitter hashtag sentiment")
findScore <- function(sentence, pos.words, neg.words){
#uses words from corr.dictionaries
require("stringr")
#remove punctuation
sentence <- gsub("[[:punct:]]", "", sentence)
#remove control characters
sentence <- gsub("[[:cntrl:]]", "", sentence)
#remove digits
sentence <- gsub("\\d+", '', sentence)
tryTolower <- function(x) {
y <- NA
try_error <- tryCatch(tolower(x), error = function(e) e)
if(!inherits(try_error, "error"))
y <- tolower(x)
return(y)
}
sentence <- sapply(sentence, tryTolower)
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.indx <- pos.matches[!is.na(pos.matches)]
neg.indx <- neg.matches[!is.na(neg.matches)]
pos.matches = length(pos.indx)
neg.matches = length(neg.indx)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score <- sum(pos.matches) - sum(neg.matches)
return(list(score, pos.indx, neg.indx, sentence))
}
calcStats <- function(scores){
ntot <- length(scores)
npos <- sum(scores > 0)
nzero <- sum(scores == 0)
nneg <- sum(scores < 0)
if (npos != 0)
avpos <- sum(scores[scores > 0]) / npos
else avpos <- 0
if (nneg != 0)
avneg <- sum(scores[scores < 0]) / nneg
else avneg <- 0
return (c(ntot, npos, nzero, nneg, avpos, avneg))
}
setup_twitter_oauth("BfoP1LtBMdghX1ULzfHNI93zI","wNPYpa0hyRQHbAa5cce8KFhLz7AAArle7YK2XNt4vWU96YyYT2","168628773-lhgkkgzHG8R5ZJuagQuWnpPDyHZJmd5I4BIFwJGL","2gROpJPcJLXj1qRe3C7jK4fp7p7pzSJfkCCGU32cVjgRj")
pos <- scan("positive-words.txt", what = "character")
neg <- scan("negative-words.txt", what = "character")
keywords <- "Russia"
nmax <- 180
location <- NULL
today <- Sys.Date()
since <- as.character(today)
until <- as.character(today + 1)
tweets <- searchTwitter(keywords, n = nmax,  since = since, until = until, geocode = location, lang = "en")
setup_twitter_oauth("xj5dwldlOwbxAy4GV0BFU3Y7C","emqDnvVHnxfWX2HpZ9K4VDarJvRQztwrpJvQ2aruKcydboAik1","168628773-lhgkkgzHG8R5ZJuagQuWnpPDyHZJmd5I4BIFwJGL","2gROpJPcJLXj1qRe3C7jK4fp7p7pzSJfkCCGU32cVjgRj")
tweets <- searchTwitter(keywords, n = nmax,  since = since, until = until, geocode = location, lang = "en")
=======
library("forecast")
library("quantmod") # загрузка с finance.google.com
library("sophisthse") # загрузка с sophist.hse.ru
install.packages("sophisthse")
library("devtools")
install_github("bdemeshev/sophisthse")
library("sophisthse") # загрузка с sophist.hse.ru
library("devtools")
install_github("bdemeshev/sophisthse")
library("devtools")
install_github("bdemeshev/sophisthse")
library("devtools")
install_github("bdemeshev/sophisthse")
library("devtools")
install_github("bdemeshev/sophisthse")
library("sophisthse") # загрузка с sophist.hse.ru
library("lubridate") # работа с датами
library("zoo") # временные ряды
install.packages("lubridate")
library("lubridate") # работа с датами
library("zoo") # временные ряды
install.packages("lubridate")
install.packages("lubridate")
install.packages("lubridate")
library("lubridate") # работа с датами
library("zoo") # временные ряды
library("xts") # еще ряды
library("dplyr") # манипуляции с данными
library("ggplot2") # графики
library("forecast")
install.packages("forecast")
library("forecast")
library("quantmod") # загрузка с finance.google.com
library("sophisthse") # загрузка с sophist.hse.ru
y <- sophisthse("CPI_M_CHI")
# индекс потребительских цен (ежемесячные данные)
y <- sophisthse("HHI_Q_I")
ed1 <- as.ts(y$HHI_Q_DIRI)
tsdisplay(as.ts(ed1))
ym <- as.ts(ed1[1:89,]) # возьмем наблюдения с 97 по последнее
tsdisplay(as.ts(ym))
mod_a <-  Arima(ym, order=c(0,1,0))
AIC(mod_a)
mod_a <- auto.arima(ym)
summary(mod_a)
AIC(mod_a)
ym <- ed1[1:89,] # возьмем наблюдения с 89 по последнее
>>>>>>> Stashed changes
